---
title: "Tim-Tuite-Bayes-Project-WR-TE"
output: html_document
---


```{r load packages, message = FALSE, include = FALSE}

#install packages if needed
if(!require(tidyverse)) install.packages('tidyverse') 
library(tidyverse)
if(!require(data.table)) install.packages('data.table')
library(data.table)
if(!require(XML)) install.packages('XML')
library(XML)
if(!require(DAAG)) install.packages('DAAG') 
library(DAAG)
if(!require(glmnet)) install.packages('glmnet') 
library(glmnet)
if(!require(outliers)) install.packages('outliers')
library(outliers)
if(!require(plyr)) install.packages('plyr')
library(plyr)
if(!require(RCurl)) install.packages('RCurl')
library(RCurl)
if(!require(rvest)) install.packages('rvest')
library(rvest)
if(!require(GGally)) install.packages('GGally')
library(GGally)
if(!require(magrittr)) install.packages('magrittr')
library(magrittr)
if(!require(broom)) install.packages('broom')
library(broom)
if(!require(rjags)) install.packages('rjags')
library(rjags)
if(!require(vctrs)) install.packages('vctrs') 
library(vctrs)
if(!require(devtools)) install.packages('devtools')
library(devtools)
#install_github("willdebras/profootballref")

```



*******************************************************************************************************************************

## Analysis   
  
The first thing we need to do is to import the data. 
```{r load real next gen stats data}

#scrape 2019 next gen data from github, I couldn't scrape this one so just downloaded it
next_gen_2019_wr_te <- read_csv("ngs_2019_receiving.csv")
next_gen_2019_wr_te$Player_Name <- paste(next_gen_2019_wr_te$player_first_name, next_gen_2019_wr_te$player_last_name)
next_gen_2019_wr_te$Player_Name <- gsub(' ', '', next_gen_2019_wr_te$Player_Name)

#per-game stats
next_gen_2019_wr_te_grouped <- next_gen_2019_wr_te %>% 
  filter(season_type == "REG") %>%
  filter(between(week, 1, 17)) %>%
  group_by(Player_Name) %>%
  filter(n() >= 8) %>%  #only include players who played in at least 8 games, since it's avgs
  summarize_at(vars(avg_cushion, avg_separation, avg_intended_air_yards, percent_share_of_intended_air_yards, receptions, targets, catch_percentage, yards, rec_touchdowns, avg_yac, avg_expected_yac, avg_yac_above_expectation), mean) %>% na.omit()


```


```{r load fantasy data}

#get the fantasy stats for 2019
fantasy_stats2019 <- profootballref::gen_tables(year = 2019) 
#cleanup the fantasy point data 
fantasy_stats2019 <- as.data.frame(cbind(fantasy_stats2019$Player, fantasy_stats2019$FantPos, fantasy_stats2019$Fantasy_FantPt, fantasy_stats2019$Fantasy_VBD, fantasy_stats2019$Fantasy_PosRank, fantasy_stats2019$Fantasy_OvRank))

#replace the special characters and spaces with blank
fantasy_stats2019$V1 <- gsub('\\*', '', fantasy_stats2019$V1)
fantasy_stats2019$V1 <- gsub('\\+', '', fantasy_stats2019$V1)
fantasy_stats2019$V1 <- gsub(' ', '', fantasy_stats2019$V1)

fantasy_stats2019[c("V3","V4", "V5", "V6")] <- sapply(fantasy_stats2019[c("V3","V4", "V5", "V6")], as.numeric)

fantasy_stats2019 <- fantasy_stats2019 %>%
  dplyr::rename(
    Player_Name = "V1",
    Fantasy_Position = "V2",
    Total_Fantasy_Points = "V3",
    Value_Over_Baseline = "V4",
    Position_Rank = "V5",
    Overall_Rank = "V6"
    )

```

  
  
```{r merge next-gen and fantasy data}

#join the fantasy and next gen data on player name
wr_te_merged_nxtgen_fantasy <- merge(next_gen_2019_wr_te_grouped, fantasy_stats2019, by = "Player_Name") 

#replace NA's in the VBD with 0 
wr_te_merged_nxtgen_fantasy$Value_Over_Baseline[is.na(wr_te_merged_nxtgen_fantasy$Value_Over_Baseline)] <- 0

head(wr_te_merged_nxtgen_fantasy)

```

Now that we've imported the data, we'll clean up the table to get it ready for further processing.


```{r exploratory data analysis - summary}

#summarize the dataset
summary(wr_te_merged_nxtgen_fantasy)


#this gives the view of the first few rows of the highest-scoring players.
wr_te_merged_nxtgen_fantasy %>% arrange(desc(Total_Fantasy_Points)) %>% head()


```

We'll also explore different correlations between different variables. 

```{r correlation}

#create a matrix with just numeric values, and ignore other dependent variables that are not fantasy points as that's not what we're predicting
wr_te_merged_nxtgen_fantasy_matrix <- as.matrix(wr_te_merged_nxtgen_fantasy %>% select(avg_cushion, avg_separation, avg_intended_air_yards, percent_share_of_intended_air_yards, receptions, targets, catch_percentage, yards, rec_touchdowns, avg_yac, avg_expected_yac, avg_yac_above_expectation, Total_Fantasy_Points))

#create a correlation matrix based on the above matrix 
ggcorr(wr_te_merged_nxtgen_fantasy_matrix, nbreaks = 5, geom = 'text', label_alpha = TRUE, angle = -15)

#We'll exclude yards and rec_touchdowns in the model, since these are directly related to the total fantasy points (i.e. they're part of the calculation), so we already know these are related. We're more interested in underlying factors. 

#we look at the right column to see the following variables look to be the most correlated with Total Fantasy Points (cutoff of .4):
##receptions
##targets
##percent_share_of_intended_yards
##catch_percentage

#We'll plot this association between receptions and total fantasy points just to get an idea of how one of the strongest correlations looks like
wr_te_merged_nxtgen_fantasy %>%
  ggplot(aes(receptions, Total_Fantasy_Points)) +
  geom_point(alpha = .5)

#We'll use linear regression to get a better understanding of what's needed soon. 


```



**************************************************************************************************************************
  
The first model we'd like to build is a Bayesian linear regression model to see the effects of the variables on overall points scored, using only variables with strong enough correlation to Total Fantasy Points.

We'll keep these variables receptions, targets, percent_share_of_intended_yards, catch_percentage

```{r Bayes linear regression wr1}

mod_wr_te_string_1 = " model {

    #likelihood
    for (k in 1:n) {
        Tot_Fant_Pts[k] ~ dnorm(mu[k], tau)
        mu[k] <- b[1] + 
        b[2]*recpts[k] + 
        b[3]*tgts[k] + 
        b[4]*pct_share_intended_yds[k] + 
        b[5]*catch_pct[k] 
    }
    
    #set non-informative priors on the betas
    for (p in 1:5) {
        b[p] ~ dnorm(0.0, 0.001)
    }

    #priors on the other non-beta parameters
    tau ~ dgamma(0.01, 0.01) 
    
} "

```

After writing our model, we sample from it using our data, and summarize the results: 
```{r use model wr1}

#here we setup the data that will be used in the above model
set.seed(72)
data_wr_te_jags = list(Tot_Fant_Pts = wr_te_merged_nxtgen_fantasy$Total_Fantasy_Points,  
                    recpts = wr_te_merged_nxtgen_fantasy$receptions,
                    tgts = wr_te_merged_nxtgen_fantasy$targets,
                    pct_share_intended_yds = wr_te_merged_nxtgen_fantasy$percent_share_of_intended_air_yards,
                    catch_pct = wr_te_merged_nxtgen_fantasy$catch_percentage,
                    n = nrow(wr_te_merged_nxtgen_fantasy)
                    )

params_wr_te_1 = c("b")  #these are the parameters that we're trying to estimate from the model above


#we create the model using that model string above and the data above
mod_wr_te_1 = jags.model(textConnection(mod_wr_te_string_1), data=data_wr_te_jags, n.chains=3)
update(mod_wr_te_1, 1000) # burn-in

#we simulate the model and the parameters 50000 times
mod_wr_te_1_sim = coda.samples(model=mod_wr_te_1,
                       variable.names=params_wr_te_1,
                       n.iter=50000)
mod_wr_te_1_csim = do.call(rbind, mod_wr_te_1_sim)

#we plot the traceplots of the different parameters
plot(mod_wr_te_1_sim)

#we output the summary of the model
summary(mod_wr_te_1_sim)

```




**********************************************************************************************************************

To do some further analysis (lasso regression), we'd like to scale the data so that the effect of certain variables do not overwhelm values on other variables. 

```{r scale data}

#we're going to be selecting certain variables and analyzing the effects of those variables on projected points. We have to scale the data to prevent outsized effects of certain variables. This needs to be done on the continuous (i.e. not categorical) variables...we'll exclude rush yards and rush touchdowns like in the first model
scaled_wr_te <- wr_te_merged_nxtgen_fantasy %>% mutate_each_(funs(scale(.) %>% as.vector),
                       vars = c('avg_cushion', 'avg_separation', 'avg_intended_air_yards', 'percent_share_of_intended_air_yards', 'receptions', 'targets', 'catch_percentage', 'yards', 'rec_touchdowns', 'avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation', 'Total_Fantasy_Points')) %>% select(-c(yards, rec_touchdowns))


head(scaled_wr_te)
```




Now that we've scaled the data, we can use LASSO regression to select certain values based on a budget on the sum of the coefficients. LASSO is a global optimization variable selection method, that keeps the most important coefficients and drops the less important coefficients to 0 (thus removing the variable from the model). There is a more generalized version of LASSO (Elastic Net), which we'll explore here too.
```{r variable selection}


#prepare data to be used in glmnet, by creating a predictors matrix holding the numeric variables and a response matrix for the fantasy points
wr_te_predictors <- as.matrix(scaled_wr_te[, 2:11])
wr_te_response_fantpts <- as.matrix(scaled_wr_te[, 13]) %>% `colnames<-`('Total_Fantasy_Points')


set.seed(1)

#we can use elastic net to tune alpha. The closer alpha is to 1, the more it behaves like lasso regression, which tends to be better for picking variables. The closer alpha is to 1, the more it behaves like ridge regression, which tends to be better for minimizing prediction error. We'll use R^2 as the measuure of quality for each iteration.

#####################

#run for alpha = 0
elastic_net_glm_0 <- cv.glmnet(x = wr_te_predictors, y = wr_te_response_fantpts, family = "gaussian", nfolds = 10, alpha = 0)
small_lambda_index_0 <- which(elastic_net_glm_0$lambda == elastic_net_glm_0$lambda.min)
small_lambda_betas_0 <- elastic_net_glm_0$glmnet.fit$beta[, small_lambda_index_0]


#calculate the r-squared
r2 <- elastic_net_glm_0$glmnet.fit$dev.ratio[which(elastic_net_glm_0$glmnet.fit$lambda == elastic_net_glm_0$lambda.min)]

#add results to a dataframe
elastic_net_results <- tibble(Alpha = '0', Rsquared = r2)


#####################

#run for alpha .25
elastic_net_glm_0.25 <- cv.glmnet(x = wr_te_predictors, y = wr_te_response_fantpts, family = "gaussian", nfolds = 10, alpha = 0.25)
small_lambda_index_0.25 <- which(elastic_net_glm_0.25$lambda == elastic_net_glm_0.25$lambda.min)
small_lambda_betas_0.25 <- elastic_net_glm_0.25$glmnet.fit$beta[, small_lambda_index_0.25]

#calculate the r-squared
r2_0.25 <- elastic_net_glm_0.25$glmnet.fit$dev.ratio[which(elastic_net_glm_0.25$glmnet.fit$lambda == elastic_net_glm_0.25$lambda.min)]

#create an entry in the R^2 table for this method
elastic_net_results <- bind_rows(elastic_net_results,
                          tibble(Alpha = '0.25', Rsquared = r2_0.25))


#####################

#run for alpha .5
elastic_net_glm_0.5 <- cv.glmnet(x = wr_te_predictors, y = wr_te_response_fantpts, family = "gaussian", nfolds = 10, alpha = 0.5)
small_lambda_index_0.5 <- which(elastic_net_glm_0.5$lambda == elastic_net_glm_0.5$lambda.min)
small_lambda_betas_0.5 <- elastic_net_glm_0.5$glmnet.fit$beta[, small_lambda_index_0.5]


#calculate the r-squared
r2_0.5 <- elastic_net_glm_0.5$glmnet.fit$dev.ratio[which(elastic_net_glm_0.5$glmnet.fit$lambda == elastic_net_glm_0.5$lambda.min)]


#add results to table
elastic_net_results <- bind_rows(elastic_net_results,
                          tibble(Alpha = '0.5', Rsquared = r2_0.5))


#####################

#run for alpha .75
elastic_net_glm_0.75 <- cv.glmnet(x = wr_te_predictors, y = wr_te_response_fantpts, family = "gaussian", nfolds = 10, alpha = 0.75)
small_lambda_index_0.75 <- which(elastic_net_glm_0.75$lambda == elastic_net_glm_0.75$lambda.min)
small_lambda_betas_0.75 <- elastic_net_glm_0.75$glmnet.fit$beta[, small_lambda_index_0.75]


#calculate the r-squared
r2_0.75 <- elastic_net_glm_0.75$glmnet.fit$dev.ratio[which(elastic_net_glm_0.75$glmnet.fit$lambda == elastic_net_glm_0.75$lambda.min)]


#add results to table
elastic_net_results <- bind_rows(elastic_net_results,
                          tibble(Alpha = '0.75', Rsquared = r2_0.75))



####################

#run for alpha 1
elastic_net_glm_1 <- cv.glmnet(x = wr_te_predictors, y = wr_te_response_fantpts, family = "gaussian", nfolds = 10, alpha = 1)
small_lambda_index_1 <- which(elastic_net_glm_1$lambda == elastic_net_glm_1$lambda.min)
small_lambda_betas_1 <- elastic_net_glm_1$glmnet.fit$beta[, small_lambda_index_1]


#calculate the r-squared
r2_1 <- elastic_net_glm_1$glmnet.fit$dev.ratio[which(elastic_net_glm_1$glmnet.fit$lambda == elastic_net_glm_1$lambda.min)]


#add results to table
elastic_net_results <- bind_rows(elastic_net_results,
                          tibble(Alpha = '1', Rsquared = r2_1))

elastic_net_results %>% knitr::kable()


#############
#From this, we see the best R-squared is when alpha = 0. We'll output this model's coefficients:
dput(sort(small_lambda_betas_0))

```

From this, we see the best R^2 value is when alpha = 0. This is a global variable selection approach which selects which variables are most useful, subject to a constraint of how much can be allocated to each variable. Values with 0 or very close to 0 can definitely be removed from the model, using |0.1| as a cutoff. That leaves these variables:  
- receptions  
- targets  
- percent_share_of_intended_air_yards  
- catch_percentage  
- avg_intended_air_yards

  
```{r Bayes linear regression}

mod_wr_te_string_2 = " model {

    #likelihood
    for (k in 1:n) {
        Tot_Fant_Pts[k] ~ dnorm(mu[k], tau)
        mu[k] <- b[1] + 
        b[2]*recpts[k] +
        b[3]*tgts[k] +
        b[4]*pct_share_intended_air_yds[k] +
        b[5]*catch_pct[k] +
        b[6]*avg_intended_air_yds[k]
    }
    
    #set non-informative priors on the betas
    for (p in 1:6) {
        b[p] ~ dnorm(0.0, 0.001)
    }

    #priors on the other non-beta parameters
    tau ~ dgamma(0.01, 0.01) 
    
} "

```

After writing our model, we sample from it using our data, and summarize the results: 
```{r use model}

#here we setup the data that will be used in the above model
set.seed(72)
data_wr_te_jags_2 = list(Tot_Fant_Pts = wr_te_merged_nxtgen_fantasy$Total_Fantasy_Points,  
                    recpts = wr_te_merged_nxtgen_fantasy$receptions,
                    tgts = wr_te_merged_nxtgen_fantasy$targets,
                    pct_share_intended_air_yds = wr_te_merged_nxtgen_fantasy$percent_share_of_intended_air_yards,
                    catch_pct = wr_te_merged_nxtgen_fantasy$catch_percentage,
                    avg_intended_air_yds = wr_te_merged_nxtgen_fantasy$avg_intended_air_yards,
                    n = nrow(wr_te_merged_nxtgen_fantasy)
                    )

params_wr_te_2 = c("b")  #these are the parameters that we're trying to estimate from the model above


#we create the model using that model string above and the data above
mod_wr_te_2 = jags.model(textConnection(mod_wr_te_string_2), data=data_wr_te_jags_2, n.chains=3)
update(mod_wr_te_2, 1000) # burn-in

#we simulate the model and the parameters 5000 times
mod_wr_te_2_sim = coda.samples(model=mod_wr_te_2,
                       variable.names=params_wr_te_2,
                       n.iter=50000)
mod_wr_te_2_csim = do.call(rbind, mod_wr_te_2_sim)

#we plot the traceplots of the different parameters
plot(mod_wr_te_2_sim)

#we output the summary of the model
summary(mod_wr_te_2_sim)

```
  
**********************************************************************************************************

**********************************************************************************************************

I also attempted Bayesian variable selection through Stochastic Search Variable Selection to identify which parameters are most likely to be important for the model by seeing which model is most visited by the Gibbs sampler:
```{r bayesian variable selection}
#https://darrenjw.wordpress.com/2012/11/20/getting-started-with-bayesian-variable-selection-using-jags-and-rjags/

#setup our data as the scaled wr/te data
data_jags_wr_te_varselect =list(y=scaled_wr_te[,13],
                     X=scaled_wr_te[,2:11],
                     n=nrow(scaled_wr_te),
                     p=ncol(scaled_wr_te[,2:11]))

#initialize the matrix
in_model_matrix_wr_te <- matrix(data = NA, 
                                nrow = data_jags_wr_te_varselect$n, 
                                ncol = data_jags_wr_te_varselect$p)

model_3_wr_te_varselect_string ="
  model {
    for (i in 1:n) {
      y[i]~dnorm(mean[i],tau)
      mean[i]<-int+inprod(X[i,],beta)
    }
    for (j in 1:p) {
      delta[j]~dbern(0.25) #prior probability xi is in the model
      alpha[j]~dnorm(0,0.1)
      beta[j]<-delta[j]*alpha[j]
    }
    int~dnorm(0,0.0001)
    tau~dgamma(1,0.001)
    #pind~dbeta(1,2)  #hyperprior on the prior probability of variable in model. beta with mean .33
    
    
    #get the model sequence, where 2 means variable in the model, as per Haldssvs.odc unit 9 example:
    for (j1 in 1:2){for(j2 in 1:2){for(j3 in 1:2){for(j4 in 1:2){for(j5 in 1:2){for(j6 in 1:2){for(j7 in 1:2){for(j8 in 1:2){for(j9 in 1:2){for(j10 in 1:2){
    in_model_matrix_wr_te[j1, j2, j3, j4, j5, j6, j7, j8, j9, j10] <- ifelse(delta[1] == j1-1, 1, 0)*ifelse(delta[2] == j2-1, 1, 0)*ifelse(delta[3] == j3-1, 1, 0)*ifelse(delta[4] == j4-1, 1, 0)*ifelse(delta[5] == j5-1, 1, 0)*ifelse(delta[6] == j6-1, 1, 0)* ifelse(delta[7] == j7-1, 1, 0)* ifelse(delta[8] == j8-1, 1, 0)* ifelse(delta[9] == j9-1, 1, 0)*ifelse(delta[9] == j10-1, 1, 0) 
    }}}}}}}}}}
   }
"
model_3_wr_te_varselect =jags.model(textConnection(model_3_wr_te_varselect_string),
                data=data_jags_wr_te_varselect,
                n.chains = 2)
update(model_3_wr_te_varselect,n.iter=50)
output_wr_te = coda.samples(model=model_3_wr_te_varselect,
        variable.names=c("delta", "in_model_matrix_wr_te"),
        n.iter=10000,thin=1)
summary_wr_te_var_selection <- summary(output_wr_te)
#plot(output)

#the means of the deltas say how often the variable was 1 (included int he model)
```

Checking the results, searching for the row with the largest mean value of in_model_matrix_wr_te, which has a probability of 0.48935).  

The model visits this model 48.935% of the time.  

This uses parameters 4 and 5 in the model (where the index value is 2).
```{r}

head(sort(summary_wr_te_var_selection$statistics[,c('Mean')], TRUE), 10)
#  delta[5]                                   delta[4] 
#                                    0.94345                                    0.58485 
# in_model_matrix_wr_te[1,1,1,2,2,1,1,1,1,1]                                   delta[3] 
#                                    0.48935                                    0.40115 
# in_model_matrix_wr_te[1,1,2,1,2,1,1,1,1,1]                                   delta[6] 
#                                    0.32635                                    0.07920 
#                                   delta[7]                                  delta[10] 
#                                    0.04775                                    0.02405 
#                                   delta[8]                                   delta[2] 
#                                    0.02320                                    0.02310 

# head(sort(summary_qb_var_selection$statistics, TRUE))
# #[1] 0.5380000 0.4988034 0.4525879 0.3827041 0.3801572 0.3239505
# sort(summary_qb_var_selection$statistics, TRUE)[2]
# #[1] 0.4988034
# which(summary_qb_var_selection$statistics == sort(summary_qb_var_selection$statistics,TRUE)[2])[1]
# #[1] 131101
```


We'll run the bayes regression model based on those 2 parameters + the intercept
```{r Bayes linear reg after ssvs}

mod_wr_te_string_3 = " model {

    #likelihood
    for (k in 1:n) {
        Tot_Fant_Pts[k] ~ dnorm(mu[k], tau)
        mu[k] <- b[1] + 
        b[2]*pct_shr_intd_air_yds[k] +
        b[3]*rcpts[k]
    }
    
    #set non-informative priors on the betas
    for (p in 1:3) {
        b[p] ~ dnorm(0.0, 0.001)
    }

    #priors on the other non-beta parameters
    tau ~ dgamma(0.01, 0.01) 
    
} "

set.seed(72)
data_wr_te_jags_3 = list(Tot_Fant_Pts = wr_te_merged_nxtgen_fantasy$Total_Fantasy_Points,  
                    pct_shr_intd_air_yds = wr_te_merged_nxtgen_fantasy$percent_share_of_intended_air_yards,
                    rcpts = wr_te_merged_nxtgen_fantasy$receptions,
                    n = nrow(wr_te_merged_nxtgen_fantasy)
                    )

params_wr_te_3 = c("b")  #these are the parameters that we're trying to estimate from the model above


#we create the model using that model string above and the data above
mod_wr_te_3 = jags.model(textConnection(mod_wr_te_string_3), data=data_wr_te_jags_3, n.chains=3)
update(mod_wr_te_3, 1000) # burn-in

#we simulate the model and the parameters 5000 times
mod_wr_te_3_sim = coda.samples(model=mod_wr_te_3,
                       variable.names=params_wr_te_3,
                       n.iter=50000)
mod_wr_te_3_csim = do.call(rbind, mod_wr_te_3_sim)

#we plot the traceplots of the different parameters
plot(mod_wr_te_3_sim)

#we output the summary of the model
summary(mod_wr_te_3_sim)


```


```{r measure model DIC}
dic_wr_te_mod1 <- dic.samples(mod_wr_te_1, n.iter = 1000)
dic_wr_te_mod2 <- dic.samples(mod_wr_te_2, n.iter = 1000)
dic_wr_te_mod3 <- dic.samples(mod_wr_te_3, n.iter = 1000)
print(dic_wr_te_mod1)
print(dic_wr_te_mod2)
print(dic_wr_te_mod3)

```

